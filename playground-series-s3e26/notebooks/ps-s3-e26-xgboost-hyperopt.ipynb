{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Play Ground Series S3 E26\n",
    "\n",
    "## ðŸ’‰ Predicting outcomes of patients with cirrhosis ðŸ’‰\n",
    "\n",
    "### Welcome to my kaggle notebook! For this Episode of the Series, is to use a multi-class approach to predict the the outcomes of patients with cirrhosis.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: For this Episode of the Series, is to use a multi-class approach to predict the the outcomes of patients with cirrhosis\n",
    "i think this is my most indepth notebook yet, i have tried to explain everything i did in detail, i hope you enjoy it and learn something from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#feature engineering then outliar detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# ML imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"..\\data\\train.csv\")\n",
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "df_original = pd.read_csv(r\"..\\data\\cirrhosis.csv\")\n",
    "\n",
    "# Rename ID column to id for consistency and concat with train data\n",
    "df_original = df_original.rename(columns={\"ID\":\"id\"})\n",
    "\n",
    "#combine train and original data\n",
    "combined_train_df = pd.concat([df_train, df_original],ignore_index=True)\n",
    "combined_train_df = combined_train_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values after combining train and original data\n",
    "\n",
    "display(df_test.head())\n",
    "display(df_original.head())\n",
    "display(combined_train_df.head(10))\n",
    "display(combined_train_df.isna().sum())\n",
    "display(combined_train_df.value_counts('Stage'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "N_Days             0\n",
       "Drug             106\n",
       "Age                0\n",
       "Sex                0\n",
       "Ascites          106\n",
       "Hepatomegaly     106\n",
       "Spiders          106\n",
       "Edema              0\n",
       "Bilirubin          0\n",
       "Cholesterol        0\n",
       "Albumin            0\n",
       "Copper             0\n",
       "Alk_Phos           0\n",
       "SGOT               0\n",
       "Tryglicerides      0\n",
       "Platelets          0\n",
       "Prothrombin        0\n",
       "Stage              0\n",
       "Status             0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "3.0    3311\n",
       "4.0    2848\n",
       "2.0    1744\n",
       "1.0     418\n",
       "2.4       1\n",
       "3.6       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# impute missing numerical values with knn imputer\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numerical_columns = combined_train_df.select_dtypes(include=[np.number]).columns\n",
    "combined_train_df_imputed = combined_train_df.copy()\n",
    "combined_train_df_imputed[numerical_columns] = imputer.fit_transform(combined_train_df[numerical_columns])\n",
    "display(combined_train_df_imputed.isna().sum())\n",
    "display(combined_train_df_imputed.value_counts('Stage'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage\n",
       "3    3311\n",
       "4    2849\n",
       "2    1745\n",
       "1     418\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_df_imputed['Stage'] = combined_train_df_imputed['Stage'].round().astype(int)\n",
    "combined_train_df_imputed.value_counts('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "N_Days           0\n",
       "Drug             0\n",
       "Age              0\n",
       "Sex              0\n",
       "Ascites          0\n",
       "Hepatomegaly     0\n",
       "Spiders          0\n",
       "Edema            0\n",
       "Bilirubin        0\n",
       "Cholesterol      0\n",
       "Albumin          0\n",
       "Copper           0\n",
       "Alk_Phos         0\n",
       "SGOT             0\n",
       "Tryglicerides    0\n",
       "Platelets        0\n",
       "Prothrombin      0\n",
       "Stage            0\n",
       "Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute missing values with most frequent value\n",
    "\n",
    "for col in combined_train_df_imputed.select_dtypes(include=['object']).columns:\n",
    "    most_frequent = combined_train_df_imputed[col].mode()[0]\n",
    "    combined_train_df_imputed[col].fillna(most_frequent, inplace=True)\n",
    "combined_train_df_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_df(df):\n",
    "    \"\"\"\n",
    "    Optimizes a pandas DataFrame's memory usage by downcasting \n",
    "    numerical columns to the smallest possible types and converting \n",
    "    object columns to categorical if appropriate.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to optimize.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The optimized DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Downcast numerical columns to save memory\n",
    "    int_columns = df.select_dtypes(include=['int64']).columns\n",
    "    float_columns = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "    # Downcast int64 columns to the smallest integer dtype\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    # Downcast float64 columns to the smallest float dtype\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # # Convert object columns to categorical if they have a relatively low cardinality\n",
    "    # object_columns = df.select_dtypes(include=['object']).columns\n",
    "    # for col in object_columns:\n",
    "    #     num_unique_values = len(df[col].unique())\n",
    "    #     num_total_values = len(df[col])\n",
    "    #     if num_unique_values / num_total_values < 0.5:  # Threshold to decide if the column should be categorical\n",
    "    #         df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "combined_train_df_imputed = optimize_df(combined_train_df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe(df):\n",
    "    \"\"\"\n",
    "    Analyze a pandas DataFrame and provide a summary of its characteristics.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"DataFrame Information:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.info(verbose=True, show_counts=True))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"DataFrame Head:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Tail:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.tail())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Description:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.describe().T)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Number of Null Values:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.isnull().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Number of Duplicated Rows:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.duplicated().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Number of Unique Values:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.nunique())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Shape:\")\n",
    "    print(\"______________________\")\n",
    "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "    \n",
    "\n",
    "analyze_dataframe(combined_train_df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_df_imputed.value_counts('Stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data points interpretation that needed further explanation\n",
    "___\n",
    "\n",
    "1. **Ascites**: Ascites is a medical term for an abnormal fluid buildup in the abdominal cavity, specifically in the peritoneal space. This syndrome is frequently caused by underlying health conditions such as liver disease, cirrhosis, heart failure, cancer, or infections.\n",
    "\n",
    "2. **Hepatomegaly**: Hepatomegaly refers to an enlargement of the liver. It is a non-specific medical sign, having many causes, which can broadly be broken down into infection, hepatic tumors, and metabolic disorder.\n",
    "\n",
    "3. **Spiders**: In medical terms, 'spiders' could refer to spider angiomas (also known as spider nevi), which are common skin lesions consisting of central arterioles surrounded by many smaller vessels due to high estrogen levels and may occur in any condition with high estrogen states such as cirrhosis.\n",
    "\n",
    "4. **Edema**: Edema is swelling caused by fluid trapped in your bodyâ€™s tissues, most often in your feet, ankles, and legs. Your healthcare provider will test your edema by pressing their finger into the swollen area (pitting) to identify how much fluid is in your tissues (grade).\n",
    "\n",
    "5. **Bilirubin**: Bilirubin is a reddish-yellow water-insoluble pigment that is formed by the breakdown of heme, is excreted in a water-soluble form by liver cells into bile, and occurs in blood and urine especially in diseased states.\n",
    "\n",
    "6. **Cholesterol**: Cholesterol is a waxy, fat-like substance that's found in all the cells in your body. Your body needs some cholesterol to make hormones, vitamin D, and substances that help you digest foods.\n",
    "\n",
    "7. **Albumin**: Albumin is a type of protein that is found in your blood. It's produced by your liver and serves several important functions in the body. One of its main roles is to help maintain the right amount of water in your blood and tissues.\n",
    "\n",
    "8. **Copper**: Copper, an essential mineral, is naturally present in some foods and is available as a dietary supplement. It is a cofactor for several enzymes (known as cuproenzymes) involved in energy production, iron metabolism, \n",
    "neuropeptide activation, connective tissue synthesis, and neurotransmitter synthesis.\n",
    "\n",
    "9. **Alk_Phos**: Alkaline phosphatase (ALP) is an enzyme thatâ€™s found throughout your body. An enzyme is a type of protein in a cell that acts as a catalyst and allows certain bodily processes to happen.\n",
    "\n",
    "10. **SGOT**: Serum glutamic oxaloacetic transaminase (SGOT), also known as aspartate aminotransferase (AST), is an enzyme that is normally present in liver and heart cells. SGOT is released into blood when the liver or heart is damaged.\n",
    "\n",
    "11. **Triglycerides**: Triglycerides are a type of fat (lipid) found in your blood. When you eat, your body converts any calories it doesn't need to use right away into triglycerides. The triglycerides are stored in your fat cells. Later, hormones release triglycerides for energy between meals.\n",
    "\n",
    "## Normal values of the data points\n",
    "____\n",
    "1. **Bilirubin**:\n",
    "   - Total: 0.1 to 1.2 mg/dL\n",
    "\n",
    "2. **Cholesterol**:\n",
    "   - Total: Less than 200 mg/dL\n",
    "\n",
    "3. **Albumin**:\n",
    "   - 3.4 to 5.4 g/dL\n",
    "\n",
    "4. **Copper**:\n",
    "   - 70 to 150 mcg/dL\n",
    "\n",
    "5. **Alkaline Phosphatase (Alk Phos)**:\n",
    "   - 44 to 147 IU/L\n",
    "\n",
    "6. **SGOT (AST)**:\n",
    "   - 10 to 40 units/L\n",
    "\n",
    "7. **Triglycerides**:\n",
    "   - Less than 150 mg/dL\n",
    "\n",
    "8. **Platelets**:\n",
    "   - 150,000 to 450,000 platelets mml/L\n",
    "\n",
    "9. **Prothrombin Time (PT)**:\n",
    "   - 11 to 13.5 seconds\n",
    "\n",
    "\n",
    "\n",
    "## Possible interactions of the data points\n",
    "___\n",
    "1. **Bilirubin and Albumin**: These are both liver function tests. Bilirubin levels can increase due to liver dysfunction, while albumin levels can decrease. Their interaction might be indicative of the severity of liver disease.\n",
    "\n",
    "2. **Ascites and Edema with Albumin**: Ascites and edema are clinical signs of advanced liver disease and can be influenced by the level of albumin, as it plays a crucial role in maintaining oncotic pressure in the blood vessels.\n",
    "\n",
    "3. **Alkaline Phosphatase (Alk_Phos) and Bilirubin**: Both are markers of liver function. Elevated levels can indicate cholestasis or blockage of bile flow, often seen in liver diseases.\n",
    "\n",
    "4. **SGOT and Bilirubin**: SGOT is an enzyme that can be elevated in liver damage. Together with bilirubin, these levels can indicate the extent of liver injury.\n",
    "\n",
    "5. **Platelets and Prothrombin**: Both are involved in blood clotting. Liver disease can lead to thrombocytopenia (low platelet count) and prolonged prothrombin time, reflecting impaired liver synthesis function.\n",
    "\n",
    "6. **Cholesterol and Triglycerides**: These are both lipids, and their interaction can be relevant in understanding the overall lipid profile, which is important in the context of cardiovascular risk and metabolic health.\n",
    "\n",
    "7. **Drug (D-penicillamine) with Liver Function Tests**: The interaction between the use of D-penicillamine and liver function tests like Bilirubin, Albumin, Alk Phos might be insightful, especially in patients with Wilsonâ€™s Disease or liver involvement in Rheumatoid Arthritis.\n",
    "\n",
    "8. **Age with Various Biomarkers**: Age might modulate the relationship between various biomarkers (like Bilirubin, Albumin, SGOT) and clinical outcomes.\n",
    "\n",
    "##  Data table for reference\n",
    "___\n",
    "| Variable Name | Role    | Type        | Demographic | Description                                             | Units    | Missing Values |\n",
    "|---------------|---------|-------------|-------------|---------------------------------------------------------|----------|----------------|\n",
    "| ID            | ID      | Integer     |             | unique identifier                                       |          | no             |\n",
    "| N_Days        | Other   | Integer     |             | number of days between registration and the earlier of  |          | no             |\n",
    "|               |         |             |             | death, transplantation, or study analysis time in July 1986 |        |                |\n",
    "| Status        | Target  | Categorical |             | status of the patient (C censored,                     |          | no             |\n",
    "|               |         |             |             | CL (censored due to liver tx), or D (death))            |          |                |\n",
    "| Drug          | Feature | Categorical |             | type of drug D-penicillamine or placebo                |          | yes            |\n",
    "| Age           | Feature | Integer     | Age         | age                                                     | days     | no             |\n",
    "| Sex           | Feature | Categorical | Sex         | M (male) or F (female)                                  |          | no             |\n",
    "| Ascites       | Feature | Categorical |             | presence of ascites N (No) or Y (Yes)                   |          | yes            |\n",
    "| Hepatomegaly  | Feature | Categorical |             | presence of hepatomegaly N (No) or Y (Yes)              |          | yes            |\n",
    "| Spiders       | Feature | Categorical |             | presence of spiders N (No) or Y (Yes)                   |          | yes            |\n",
    "| Edema         | Feature | Categorical |             | presence of edema N (no edema and no diuretic therapy   |          | no             |\n",
    "|               |         |             |             | for edema), 0 (edema present without diuretics,         |          |                |\n",
    "|               |         |             |             | or edema resolved by diuretics), or Y (edema despite    |          |                |\n",
    "|               |         |             |             | diuretic therapy)                                       |          |                |\n",
    "| Bilirubin     | Feature | Continuous  |             | serum bilirubin                                         | mg/dl    | no             |\n",
    "| Cholesterol   | Feature | Integer     |             | serum cholesterol                                       | mg/dl    | yes            |\n",
    "| Albumin       | Feature | Continuous  |             | albumin                                                 | gm/dl    | no             |\n",
    "| Copper        | Feature | Integer     |             | urine copper                                            | ug/day   | yes            |\n",
    "| Alk_Phos      | Feature | Continuous  |             | alkaline phosphatase                                    | U/liter  | yes            |\n",
    "| SGOT          | Feature | Continuous  |             | SGOT                                                    | U/ml     | yes            |\n",
    "| Triglycerides | Feature | Integer     |             | triglycerides                                           |          | yes            |\n",
    "| Platelets     | Feature | Integer     |             | platelets per cubic                                     | ml/1000  | yes            |\n",
    "| Prothrombin   | Feature | Continuous  |             | prothrombin time                                        | s        | yes            |\n",
    "| Stage         | Feature | Categorical |             | histologic stage of disease (1, 2, 3, or 4)             |          | yes            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column variables and normal ranges to print out  for EDA\n",
    "\n",
    "normal_ranges = {\n",
    "    \"Bilirubin\": (0.3, 1.2),\n",
    "    \"Cholesterol\": (0, 200),\n",
    "    \"Albumin\": (3.4, 5.4),\n",
    "    \"Copper\": (70, 140),\n",
    "    \"Alk_Phos\": (44, 147),\n",
    "    \"SGOT\": (10, 40),\n",
    "    \"Tryglicerides\": (0, 150),\n",
    "    \"Platelets\": (150, 450),\n",
    "    \"Prothrombin\": (9.5, 13.5)\n",
    "}\n",
    "\n",
    "columns_to_plot = [\n",
    "    \"Bilirubin\",\n",
    "    \"Cholesterol\",\n",
    "    \"Albumin\",\n",
    "    \"Copper\",\n",
    "    \"Alk_Phos\",\n",
    "    \"SGOT\",\n",
    "    \"Tryglicerides\",\n",
    "    \"Platelets\",\n",
    "    \"Prothrombin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prelim_eda_histplot(df):\n",
    "    \"\"\"\n",
    "    Create a histogram plot of each column in a pandas DataFrame using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_to_drop = ['id','Drug','Sex','Ascites','Hepatomegaly','Spiders','Edema','Stage','Status']\n",
    "    columns_to_plot = [column for column in df.columns if column not in columns_to_drop]\n",
    "    \n",
    "\n",
    "    num_columns = len(columns_to_plot)\n",
    "    num_rows = (num_columns - 1) // 4 + 1  # Calculate the number of rows based on the number of columns\n",
    "    num_cols = min(num_columns, 4)  # Set the number of columns to 4 or the number of columns in the DataFrame, whichever is smaller\n",
    "\n",
    "    # Create a subplot grid\n",
    "    fig = make_subplots(rows=num_rows, cols=num_cols, subplot_titles=columns_to_plot)\n",
    "\n",
    "    # Loop over selected columns and create histogram plots in separate subplots\n",
    "    for i, column in enumerate(columns_to_plot, start=1):\n",
    "        row = (i - 1) // num_cols + 1\n",
    "        col = (i - 1) % num_cols + 1\n",
    "        trace = go.Histogram(x=df[column], nbinsx=20, name=column)\n",
    "        fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.update_layout(height=300*num_rows, width=300*num_cols, title_text=\"Histograms of DataFrame Columns\", showlegend=False, title_x=0.5)\n",
    "    fig.update_traces(opacity=.75)\n",
    "    fig.show()\n",
    "\n",
    "# Example usage\n",
    "prelim_eda_histplot(combined_train_df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prelim_eda_histplot(df):\n",
    "    \"\"\"\n",
    "    Create a histogram plot of each column in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    columns_to_plot = df.drop(['id','Drug','Sex','Ascites','Hepatomegaly','Spiders','Edema','Stage','Status'], axis=1)      \n",
    "\n",
    "\n",
    "    num_columns = len(columns_to_plot.columns)\n",
    "    num_rows = (num_columns - 1) // 4 + 1  # Calculate the number of rows based on the number of columns\n",
    "    num_cols = min(num_columns, 4)  # Set the number of columns to 4 or the number of columns in the DataFrame, whichever is smaller\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 12))  # Adjust the figsize as needed\n",
    "\n",
    "    # Loop over selected columns and create histogram plots in separate subplots\n",
    "    for i, column in enumerate(columns_to_plot):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        sns.histplot(data=df, x=column, ax=axes[row, col], kde=True, bins=20)\n",
    "        axes[row, col].set_title(f'{column}', fontsize=14)\n",
    "        axes[row, col].set_aspect('auto')\n",
    "\n",
    "    # Remove empty subplots (if any)\n",
    "    for i in range(num_columns, num_rows * num_cols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "prelim_eda_histplot(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_boxplots(df, exclude_columns):\n",
    "    \"\"\"\n",
    "    Create a series of boxplot subplots for the given DataFrame, excluding specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    exclude_columns (list): List of column names to exclude from plotting.\n",
    "\n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The figure object containing the subplots.\n",
    "    \"\"\"\n",
    "    # Dropping the specified columns\n",
    "    df_to_plot = df.drop(exclude_columns, axis=1)\n",
    "    num_cols = len(df_to_plot.columns)\n",
    "    \n",
    "    # Determining the layout of the subplot grid\n",
    "    cols_per_row = 3\n",
    "    rows = (num_cols + cols_per_row - 1) // cols_per_row\n",
    "\n",
    "    # Creating the subplots\n",
    "    fig, axes = plt.subplots(rows, cols_per_row, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten()  # Flatten in case of a single row\n",
    "\n",
    "    for i, col in enumerate(df_to_plot.columns):\n",
    "        df_to_plot.boxplot(column=col, ax=axes[i])\n",
    "        axes[i].set_title(col)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, rows * cols_per_row):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Columns to exclude from the plot\n",
    "exclude_columns = ['id', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage', 'Status']\n",
    "\n",
    "# Creating the boxplot subplots\n",
    "fig = create_boxplots(combined_train_df_imputed, exclude_columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_boxplot_subplots(df, columns_to_plot, normal_ranges):\n",
    "    \"\"\"\n",
    "    Create a subplot of box plots for the specified columns in the given DataFrame,\n",
    "    including horizontal lines for normal ranges and corresponding legend entries.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data.\n",
    "    columns_to_plot (list): List of column names to plot.\n",
    "    normal_ranges (dict): Dictionary of the form {column_name: (lower, upper)} representing normal ranges.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=3, cols=3, subplot_titles=columns_to_plot)\n",
    "\n",
    "    # Add a box plot to each subplot cell and normal range lines\n",
    "    for i, column in enumerate(columns_to_plot, start=1):\n",
    "        row = (i - 1) // 3 + 1\n",
    "        col = (i - 1) % 3 + 1\n",
    "        fig.add_trace(go.Box(y=df[column], name=column), row=row, col=col)\n",
    "        \n",
    "        # Add normal range lines if available\n",
    "        if column in normal_ranges:\n",
    "            lower, upper = normal_ranges[column]\n",
    "            # Lower normal range line\n",
    "            fig.add_shape(type='line',\n",
    "                          xref='x' + str(i), yref='y' + str(i),\n",
    "                          x0=0, x1=1, y0=lower, y1=lower,\n",
    "                          line=dict(color='green', width=2),\n",
    "                          row=row, col=col)\n",
    "            # Upper normal range line\n",
    "            fig.add_shape(type='line',\n",
    "                          xref='x' + str(i), yref='y' + str(i),\n",
    "                          x0=0, x1=1, y0=upper, y1=upper,\n",
    "                          line=dict(color='red', width=2),\n",
    "                          row=row, col=col)\n",
    "    \n",
    "    # Add invisible traces for legend entries\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines',\n",
    "                             line=dict(color='green', width=2), name='Lower Normal Limit'))\n",
    "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines',\n",
    "                             line=dict(color='red', width=2), name='Upper Normal Limit'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        width=1000,\n",
    "        title_text=\"Laboratory Measurements Distribution\",\n",
    "        font_size=14,\n",
    "        title_x=0.5,\n",
    "        legend=dict(bordercolor=\"Black\", borderwidth=2)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "create_boxplot_subplots(combined_train_df_imputed, columns_to_plot, normal_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_plotly_express_heatmap(df, columns_to_plot, color_continuous_scale='viridis'):\n",
    "    \"\"\"\n",
    "    Create a heatmap for the correlation matrix of specified columns in the given DataFrame using Plotly Express.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    columns_to_plot (list): List of column names to include in the heatmap.\n",
    "    color_continuous_scale (str or list): Colormap used for the heatmap.\n",
    "\n",
    "    Returns:\n",
    "    plotly.graph_objs._figure.Figure: The figure object containing the heatmap.\n",
    "    \"\"\"\n",
    "    # Select specified columns for correlation matrix\n",
    "    df_subset = df[columns_to_plot]\n",
    "    correlation_matrix = df_subset.corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig = px.imshow(\n",
    "        correlation_matrix,\n",
    "        labels=dict(x=\"Variable\", y=\"Variable\", color=\"Correlation\"),\n",
    "        x=columns_to_plot,\n",
    "        y=columns_to_plot,\n",
    "        color_continuous_scale=color_continuous_scale,\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        title=\"Heatmap of Correlation Matrix\",\n",
    "        template='plotly_white',\n",
    "        color_continuous_midpoint=0\n",
    "    )\n",
    "\n",
    "    # Add annotations\n",
    "    for i in range(len(columns_to_plot)):\n",
    "        for j in range(len(columns_to_plot)):\n",
    "            fig.add_annotation(\n",
    "                x=columns_to_plot[i],\n",
    "                y=columns_to_plot[j],\n",
    "                text=f\"{correlation_matrix.iloc[i, j]:.2f}\",\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"black\", size=12),\n",
    "            )\n",
    "\n",
    "    # Update the layout\n",
    "    fig.update_layout(\n",
    "        title='Heatmap of Correlation Matrix',\n",
    "        xaxis_nticks=len(columns_to_plot),\n",
    "        yaxis_nticks=len(columns_to_plot),\n",
    "        title_x=0.5\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "create_plotly_express_heatmap(df_train, columns_to_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(df, figsize=(10, 8), annot=True, cmap='coolwarm',fmt='.1f'):\n",
    "    \"\"\"\n",
    "    Create a heatmap for the correlation matrix of the numeric columns in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    figsize (tuple): Size of the heatmap.\n",
    "    annot (bool): If True, write the data value in each cell.\n",
    "    cmap (str): Colormap used for the heatmap.\n",
    "\n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The figure object containing the heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Select only numeric columns for correlation matrix\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "\n",
    "    sns.heatmap(correlation_matrix, annot=annot, cmap=cmap, fmt=fmt)\n",
    "    plt.title('Heatmap of Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "create_heatmap(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "____\n",
    "Some more indepth EDA on the outliers to get a better picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normal ranges for each test\n",
    "\n",
    "# Create scatter plots\n",
    "plt.figure(figsize=(25, 20))\n",
    "for i, (test, range_vals) in enumerate(normal_ranges.items(), 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    custom_palette = ['#ff7f0e', '#2ca02c', '#1f77b4', '#d62728'] \n",
    "    sns.scatterplot(x=range(len(df_train)), y=df_train[test], hue=df_train['Stage'], alpha=0.3, palette=custom_palette)\n",
    "\n",
    "    # Normal range lines\n",
    "    plt.axhline(y=range_vals[0], color='black', linestyle='--', label='Lower Normal Limit')\n",
    "    plt.axhline(y=range_vals[1], color='blue', linestyle='--', label='Upper Normal Limit')\n",
    "\n",
    "    # Quantile range lines (25th and 75th percentiles)\n",
    "    lower_quantile = df_train[test].quantile(0.25)\n",
    "    upper_quantile = df_train[test].quantile(0.75)\n",
    "    plt.axhline(y=lower_quantile, color='green', linestyle='-', label='25th Percentile')\n",
    "    plt.axhline(y=upper_quantile, color='red', linestyle='-', label='75th Percentile')\n",
    "\n",
    "    # Median line\n",
    "    median = df_train[test].median()\n",
    "    plt.axhline(y=median, color='black', linestyle='-', label='Median')\n",
    "\n",
    "\n",
    "    plt.title(test)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(test)\n",
    "    plt.legend(bbox_to_anchor=(1.28,1),loc='upper right', fontsize='small', ncol=1, framealpha=0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = make_subplots(rows=3, cols=3, subplot_titles=list(normal_ranges.keys()))\n",
    "\n",
    "color_map = {\n",
    "    1: 'blue',\n",
    "    2: 'green',\n",
    "    3: 'red',\n",
    "    4: 'purple'\n",
    "}\n",
    "\n",
    "# Loop over your normal_ranges to create scatter plots and lines\n",
    "for i, (test, range_vals) in enumerate(normal_ranges.items(), start=1):\n",
    "    row = (i - 1) // 3 + 1\n",
    "    col = (i - 1) % 3 + 1\n",
    "\n",
    "    # Add scatter plot\n",
    "    for stage in df_train['Stage'].unique():\n",
    "        stage_data = df_train[df_train['Stage'] == stage]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=stage_data.index, \n",
    "                y=stage_data[test], \n",
    "                mode='markers', \n",
    "                name=f'Stage {stage}',\n",
    "                marker=dict(color=color_map[stage], opacity=0.5,size=3),\n",
    "                legendgroup=f'stage{stage}',  # Different group for each stage\n",
    "                showlegend=(i == 1)  # Only show legend for the first subplot\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    # Add normal range and quantile lines without adding to legend again\n",
    "    fig.add_hline(y=range_vals[0], line_dash='solid', line_color='black', row=row, col=col)\n",
    "    fig.add_hline(y=range_vals[1], line_dash='solid', line_color='lightblue', row=row, col=col)\n",
    "    lower_quantile = df_train[test].quantile(0.25)\n",
    "    upper_quantile = df_train[test].quantile(0.75)\n",
    "    fig.add_hline(y=lower_quantile, line_dash='solid', line_color='orange', row=row, col=col)\n",
    "    fig.add_hline(y=upper_quantile, line_dash='solid', line_color='red', row=row, col=col)\n",
    "\n",
    "# Add invisible traces for the legend for lines\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines',\n",
    "    line=dict(color='black', dash='solid'),\n",
    "    legendgroup='lines',  # Same group for all lines\n",
    "    showlegend=True, name='Lower Normal Limit'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines',\n",
    "    line=dict(color='lightblue', dash='solid'),\n",
    "    legendgroup='lines',\n",
    "    showlegend=True, name='Upper Normal Limit'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines',\n",
    "    line=dict(color='orange', dash='solid'),\n",
    "    legendgroup='lines',\n",
    "    showlegend=True, name='25th Percentile'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines',\n",
    "    line=dict(color='red', dash='solid'),\n",
    "    legendgroup='lines',\n",
    "    showlegend=True, name='75th Percentile'\n",
    "))\n",
    "\n",
    "# Update layout to adjust the size and title\n",
    "fig.update_layout(\n",
    "    height=900, \n",
    "    width=1200, \n",
    "    title_text=\"Laboratory Measurements Distribution\", \n",
    "    title_x=0.5,\n",
    "    legend_traceorder='reversed'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_train is your DataFrame and normal_ranges is your dictionary of ranges\n",
    "\n",
    "# Create a single scatter plot for demonstration\n",
    "lower_normal_color = 'black'\n",
    "upper_normal_color = 'black'\n",
    "lower_normal_dash = 'dash'\n",
    "quantile_25_color = 'blue'\n",
    "quantile_75_color = 'green'\n",
    "quantile_75_dash = 'solid'\n",
    "line_width = 2\n",
    "\n",
    "test = 'Bilirubin'\n",
    "lower_limit = normal_ranges[test][0]\n",
    "upper_limit = normal_ranges[test][1]\n",
    "\n",
    "#needed to sort these so they showed up in the legned in the correct order\n",
    "df_train_sorted = combined_train_df_imputed.sort_values(by='Stage', ascending=False)\n",
    "\n",
    "\n",
    "fig = px.scatter(df_train_sorted, x=df_train_sorted.index, y=test, title=test, opacity=0.5,color='Stage',color_discrete_map={1:'green',2:'orange',3:'blue',4:'red'})\n",
    "\n",
    "# Add horizontal lines for normal range and quantiles using variables\n",
    "fig.add_hline(y=lower_limit, line_dash='solid', line_color=lower_normal_color, line_width=line_width)\n",
    "fig.add_hline(y=upper_limit, line_dash=lower_normal_dash, line_color=upper_normal_color, line_width=line_width)\n",
    "fig.add_hline(y=lower_quantile, line_dash='solid', line_color=quantile_25_color, line_width=line_width)\n",
    "fig.add_hline(y=upper_quantile, line_dash=quantile_75_dash, line_color=quantile_75_color, line_width=line_width)\n",
    "fig.add_hline(y=median, line_dash='solid', line_color='orange', line_width=line_width)\n",
    "\n",
    "# Add invisible traces for the line legend using variables\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines', \n",
    "    line=dict(color=lower_normal_color, dash='dash', width=line_width), \n",
    "    name='Lower Normal Limit'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines', \n",
    "    line=dict(color=upper_normal_color, dash='solid', width=line_width), \n",
    "    name='Upper Normal Limit'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines',\n",
    "    line=dict(color=quantile_25_color, dash='solid', width=line_width), \n",
    "    name='25th Percentile'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines', \n",
    "    line=dict(color=quantile_75_color, dash=quantile_75_dash, width=line_width), \n",
    "    name='75th Percentile'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None], y=[None], mode='lines', \n",
    "    line=dict(color='orange', dash='solid', width=line_width), \n",
    "    name='Median'\n",
    "))\n",
    "\n",
    "# Update y-axis range to add a 10% margin on top\n",
    "y_max = df_train_sorted[test].max()\n",
    "y_margin = y_max * 0.1\n",
    "fig.update_yaxes(range=[0, y_max + y_margin])\n",
    "fig.update_traces(marker_coloraxis=None)   \n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_age_and_create_interactions(df):\n",
    "    \"\"\"\n",
    "    Convert 'Age' from days to years and create interaction features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Feature Engineering: Categorical Variable Interactions and Grouped Categorical Variables\n",
    "\n",
    "    \n",
    "    # df['Liver_Health_Indicators'] = df[['Ascites', 'Hepatomegaly', 'Spiders']].apply(lambda x: (x == 'Y').sum(), axis=1)\n",
    "\n",
    "    df['Age_years'] = df['Age'] / 365.25\n",
    "    df['Age_years'] = df['Age_years'].round().astype(int)\n",
    "    \n",
    "    # df['Age_group'] = pd.cut(df['Age_years'], bins=[25, 35, 45, 55, 65, 75, 80], labels=['1', '2', '3', '4', '5', '6'])\n",
    "    # df['Age_group'] = pd.to_numeric(df['Age_group'])\n",
    "    # df['Bilirubin_Albumin_interaction'] = df['Bilirubin'] * df['Albumin']\n",
    "    # df['Platelets_Prothrombin_interaction'] = df['Platelets'] * df['Prothrombin']\n",
    "    \n",
    "    return df\n",
    "pp_train = convert_age_and_create_interactions(combined_train_df_imputed)\n",
    "test_df = convert_age_and_create_interactions(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>N_Days</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>...</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Alk_Phos</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>Tryglicerides</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Prothrombin</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Status</th>\n",
       "      <th>Age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4933.0</td>\n",
       "      <td>2255.0</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>22646.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.35</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>92.00</td>\n",
       "      <td>209.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3353.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>12109.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.79</td>\n",
       "      <td>186.0</td>\n",
       "      <td>2115.0</td>\n",
       "      <td>136.00</td>\n",
       "      <td>149.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>353.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>22646.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.40</td>\n",
       "      <td>27.0</td>\n",
       "      <td>795.8</td>\n",
       "      <td>95.48</td>\n",
       "      <td>104.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2667.0</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>12897.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>3.64</td>\n",
       "      <td>48.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>52.70</td>\n",
       "      <td>214.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3419.0</td>\n",
       "      <td>1783.0</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>18713.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>3.37</td>\n",
       "      <td>50.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>237.15</td>\n",
       "      <td>56.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  N_Days             Drug      Age Sex Ascites Hepatomegaly Spiders  \\\n",
       "0  4933.0  2255.0  D-penicillamine  22646.0   F       N            N       N   \n",
       "1  3353.0   788.0          Placebo  12109.0   F       N            N       Y   \n",
       "2   353.0  2698.0          Placebo  22646.0   F       N            Y       N   \n",
       "3  2667.0  1735.0          Placebo  12897.0   F       N            Y       N   \n",
       "4  3419.0  1783.0          Placebo  18713.0   F       N            N       Y   \n",
       "\n",
       "  Edema  Bilirubin  ...  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n",
       "0     N        1.1  ...     3.35     9.0    2520.0   92.00          209.0   \n",
       "1     N        6.4  ...     3.79   186.0    2115.0  136.00          149.0   \n",
       "2     N        1.3  ...     3.40    27.0     795.8   95.48          104.0   \n",
       "3     N        0.6  ...     3.64    48.0     794.0   52.70          214.0   \n",
       "4     N        0.6  ...     3.37    50.0     975.0  237.15           56.0   \n",
       "\n",
       "   Platelets  Prothrombin  Stage  Status Age_years  \n",
       "0      309.0         11.0      1       C        62  \n",
       "1      200.0         10.8      4       C        33  \n",
       "2      167.0         10.6      4       C        62  \n",
       "3      271.0         10.6      3       C        35  \n",
       "4      296.0          9.9      4       C        51  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Isolation Forest to detect outliers/anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "anomaly_columns = [\n",
    "    \"Bilirubin\",\n",
    "    \"Cholesterol\",\n",
    "    \"Albumin\",\n",
    "    \"Copper\",\n",
    "    \"Alk_Phos\",\n",
    "    \"SGOT\",\n",
    "    \"Tryglicerides\",\n",
    "    \"Platelets\",\n",
    "    \"Prothrombin\",\n",
    "    \"Age_years\",\n",
    "    # \"Bilirubin_Albumin_interaction\",\n",
    "    # \"Platelets_Prothrombin_interaction\",\n",
    "]\n",
    "\n",
    "model_IF = IsolationForest(contamination=float(0.15),n_jobs=-1, random_state=42, verbose=1)\n",
    "model_IF.fit(pp_train[anomaly_columns])\n",
    "pp_train['anomaly_score_IF'] = model_IF.decision_function(pp_train[anomaly_columns])\n",
    "pp_train['anomaly_IF'] = model_IF.predict(pp_train[anomaly_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_plot(data, outlier_method, x_var, y_var, x_axis_limits=[0,1], y_axis_limits=[0,1]):\n",
    "    print(f'Outlier Method: {outlier_method}')\n",
    "    print(f'Number of outliers: {len(data[data[\"anomaly_IF\"] == -1])}')\n",
    "    print(f'Number of inliers: {len(data[data[\"anomaly_IF\"] == 1])}')\n",
    "    print(f'Number of total points: {len(data)}')\n",
    "    print(f'Percentage of outliers: {len(data[data[\"anomaly_IF\"] == -1])/len(data)}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(['#ff7f0e','#1f77b4'])\n",
    "\n",
    "sns.pairplot(df_train, vars=anomaly_columns, hue='anomaly_IF', palette=palette)\n",
    "plt.suptitle('Outlier Detection using Isolation Forest', y=1.1, fontweight='bold', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_plot(pp_train, 'IF', 'Bilirubin', 'Cholesterol', [-1, 30],[-1, 220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers\n",
    "pp_train = pp_train.drop(pp_train[pp_train['anomaly_IF'] == -1].index)\n",
    "# drop anomaly columns \n",
    "pp_train = pp_train.drop(['anomaly_score_IF','anomaly_IF'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Converting Age from days to years and creating interaction features  and liver health indicators\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8323 entries, 0 to 8322\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             8323 non-null   float64\n",
      " 1   N_Days         8323 non-null   float64\n",
      " 2   Drug           8323 non-null   object \n",
      " 3   Age            8323 non-null   float64\n",
      " 4   Sex            8323 non-null   object \n",
      " 5   Ascites        8323 non-null   object \n",
      " 6   Hepatomegaly   8323 non-null   object \n",
      " 7   Spiders        8323 non-null   object \n",
      " 8   Edema          8323 non-null   object \n",
      " 9   Bilirubin      8323 non-null   float64\n",
      " 10  Cholesterol    8323 non-null   float64\n",
      " 11  Albumin        8323 non-null   float64\n",
      " 12  Copper         8323 non-null   float64\n",
      " 13  Alk_Phos       8323 non-null   float64\n",
      " 14  SGOT           8323 non-null   float64\n",
      " 15  Tryglicerides  8323 non-null   float64\n",
      " 16  Platelets      8323 non-null   float64\n",
      " 17  Prothrombin    8323 non-null   float64\n",
      " 18  Stage          8323 non-null   int32  \n",
      " 19  Status         8323 non-null   object \n",
      " 20  Age_years      8323 non-null   int32  \n",
      "dtypes: float64(12), int32(2), object(7)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "pp_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to the dataframe.\n",
    "\n",
    "    :param df: DataFrame to be processed\n",
    "    :param features_to_transform: List of features to transform outliers in\n",
    "    :return: Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Apply preprocessing for new features\n",
    "    df = convert_age_and_create_interactions(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:49<00:00,  2.00trial/s, best loss: 0.44424217051405196]\n",
      "Best parameters:  {'colsample_bytree': 0.5031331406523033, 'gamma': 0.00012620421737027032, 'learning_rate': 0.1150005120091401, 'max_depth': 6.0, 'min_child_weight': 0.335896630953778, 'reg_alpha': 3.4441782632180264, 'reg_lambda': 6.104907325210941, 'subsample': 0.9846733008892326}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "pp_train, pp_test = train_test_split(pp_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encoding the target variable 'Status' if it's categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(pp_train['Status'])\n",
    "y_test = label_encoder.transform(pp_test['Status'])\n",
    "\n",
    "# Selecting categorical columns (excluding the target variable 'Status')\n",
    "cat_cols = pp_train.select_dtypes(include=['object']).drop(columns=['Status']).columns\n",
    "\n",
    "# Preprocessing with OneHotEncoder and StandardScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), pp_train.select_dtypes(exclude=['object']).drop(['id'], axis=1).columns),\n",
    "        ('cat', OneHotEncoder(), cat_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the training data and transform the test data\n",
    "X_train = preprocessor.fit_transform(pp_train.drop(columns=['Status', 'id']))\n",
    "X_test = preprocessor.transform(pp_test.drop(columns=['Status', 'id']))\n",
    "\n",
    "# Define the space of hyperparameters to search\n",
    "space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 1)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 8, 1)),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -2, 3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 1, 10),\n",
    "    'gamma': hp.loguniform('gamma', -10, 10),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, 0),\n",
    "    'random_state': 42,\n",
    "}\n",
    "def objective(params):\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, eval_metric='mlogloss', **params)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)\n",
    "    logloss = log_loss(y_test, y_pred_proba)  # Assuming you've imported log_loss from sklearn.metrics\n",
    "    return {'loss': logloss, 'status': STATUS_OK}\n",
    "\n",
    "# Run the hyperparameter search using the tpe algorithm\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=4000,  # Adjust based on your computation resource\n",
    "            trials=Trials())\n",
    "\n",
    "print(\"Best parameters: \", best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.01886\n",
      "[100]\tvalidation_0-mlogloss:0.44303\n",
      "[162]\tvalidation_0-mlogloss:0.44363\n",
      "Final model accuracy:  0.8312312312312312\n",
      "Final model log loss:  0.4418360456626488\n"
     ]
    }
   ],
   "source": [
    "best['max_depth'] = int(best['max_depth'])\n",
    "final_model = xgb.XGBClassifier(n_estimators = 750, objective='multi:softprob', num_class=3, eval_metric='mlogloss', **best, early_stopping_rounds=50)\n",
    "final_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100)\n",
    "\n",
    "# Predict class labels\n",
    "y_pred_class = final_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(\"Final model accuracy: \", final_accuracy)\n",
    "\n",
    "# Predict probabilities and calculate log loss\n",
    "y_pred_proba = final_model.predict_proba(X_test)\n",
    "final_logloss = log_loss(y_test, y_pred_proba)\n",
    "print(\"Final model log loss: \", final_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data using the same preprocessor\n",
    "X_new = preprocessor.transform(test_df.drop(columns=['id']))\n",
    "\n",
    "# Making predictions on the new data\n",
    "predictions = final_model.predict_proba(X_new)\n",
    "\n",
    "# Creating the DataFrame for submission\n",
    "prediction_columns = ['Status_C', 'Status_CL', 'Status_D']\n",
    "submission_df = pd.DataFrame(predictions, columns=prediction_columns)\n",
    "submission_df['id'] = test_df['id']\n",
    "\n",
    "# Reordering columns to make 'id' the first column\n",
    "submission_df = submission_df[['id'] + prediction_columns]\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "submission_df.to_csv('../data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggleS3E26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
