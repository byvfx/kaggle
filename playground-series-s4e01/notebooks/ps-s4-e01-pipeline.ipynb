{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold as SKFold, train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "FILEPATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f\"{FILEPATH}test.csv\")\n",
    "train = pd.read_csv(f\"{FILEPATH}train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe(df):\n",
    "    \"\"\"\n",
    "    Analyze a pandas DataFrame and provide a summary of its characteristics.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"DataFrame Information:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.info(verbose=True, show_counts=True))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Head:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Tail:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.tail())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Description:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.describe().T)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Number of Null Values:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.isnull().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Number of Duplicated Rows:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.duplicated().sum())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Number of Unique Values:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.nunique())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Shape:\")\n",
    "    print(\"______________________\")\n",
    "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DataFrame Columns:\")\n",
    "    print(\"______________________\")\n",
    "    display(df.columns)\n",
    "\n",
    "\n",
    "analyze_dataframe(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate vowel and consonant count\n",
    "def vowel_consonant_count(word):\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    vowel_count = sum(1 for char in word if char in vowels)\n",
    "    consonant_count = sum(1 for char in word if char not in vowels and char.isalpha())\n",
    "    return vowel_count, consonant_count\n",
    "\n",
    "def create_surname_features(df):\n",
    "    df['Length'] = df['Surname'].apply(len)\n",
    "    df['Initial'] = df['Surname'].str[0]\n",
    "    df[['Vowels', 'Consonants']] = df['Surname'].apply(lambda x: vowel_consonant_count(x)).tolist()\n",
    "    df['Uniqueness'] = df['Surname'].apply(lambda x: len(set(x.lower())) / len(x) if x else 0)\n",
    "    df['isSenior'] = df['Age'] > 60\n",
    "    \n",
    "    # Create Credit Rating\n",
    "    bins = [0,100, 300, 580, 670, 740, 800, 850]\n",
    "    labels = ['Very Poor', 'Poor', 'Fair', 'Good', 'Very Good', 'Exceptional', 'Excellent']\n",
    "    df['CreditRating'] = pd.cut(df['CreditScore'], bins=bins, labels=labels, include_lowest=True)\n",
    "    return df\n",
    "\n",
    "train = create_surname_features(train)\n",
    "test = create_surname_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Length</th>\n",
       "      <th>Initial</th>\n",
       "      <th>Vowels</th>\n",
       "      <th>Consonants</th>\n",
       "      <th>Uniqueness</th>\n",
       "      <th>isSenior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15674932</td>\n",
       "      <td>Okwudilichukwu</td>\n",
       "      <td>668</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181449.97</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15749177</td>\n",
       "      <td>Okwudiliolisa</td>\n",
       "      <td>627</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49503.50</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15694510</td>\n",
       "      <td>Hsueh</td>\n",
       "      <td>678</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184866.69</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15741417</td>\n",
       "      <td>Kao</td>\n",
       "      <td>581</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2</td>\n",
       "      <td>148882.54</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84560.88</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>K</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15766172</td>\n",
       "      <td>Chiemenam</td>\n",
       "      <td>716</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15068.83</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  CustomerId         Surname  CreditScore Geography Gender   Age  Tenure  \\\n",
       "0   0    15674932  Okwudilichukwu          668    France   Male  33.0       3   \n",
       "1   1    15749177   Okwudiliolisa          627    France   Male  33.0       1   \n",
       "2   2    15694510           Hsueh          678    France   Male  40.0      10   \n",
       "3   3    15741417             Kao          581    France   Male  34.0       2   \n",
       "4   4    15766172       Chiemenam          716     Spain   Male  33.0       5   \n",
       "\n",
       "     Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
       "0       0.00              2        1.0             0.0        181449.97   \n",
       "1       0.00              2        1.0             1.0         49503.50   \n",
       "2       0.00              2        1.0             0.0        184866.69   \n",
       "3  148882.54              1        1.0             1.0         84560.88   \n",
       "4       0.00              2        1.0             1.0         15068.83   \n",
       "\n",
       "   Exited  Length Initial  Vowels  Consonants  Uniqueness  isSenior  \n",
       "0       0      14       O       6           8    0.642857     False  \n",
       "1       0      13       O       7           6    0.692308     False  \n",
       "2       0       5       H       2           3    0.800000     False  \n",
       "3       0       3       K       2           1    1.000000     False  \n",
       "4       0       9       C       4           5    0.777778     False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop([\"Surname\", \"CustomerId\"], axis=1, errors=\"ignore\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES\n",
    "cat_features = [\n",
    "    \"Geography\",\n",
    "    \"Gender\",\n",
    "    \"HasCrCard\",\n",
    "    \"IsActiveMember\",\n",
    "    \"NumOfProducts\",\n",
    "    \"Initial\",\n",
    "    \"isSenior\",\n",
    "    \"CreditRating\",\n",
    "]\n",
    "num_features = [\n",
    "    \"CreditScore\",\n",
    "    \"Age\",\n",
    "    \"Tenure\",\n",
    "    \"EstimatedSalary\",\n",
    "    \"Uniqueness\",\n",
    "    \"Vowels\",\n",
    "    \"Consonants\",\n",
    "    \"Length\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:                             \n",
      "[0.87921879 0.87397919 0.87650899 0.87550835 0.87449643]\n",
      "100%|██████████| 1/1 [01:43<00:00, 103.54s/trial, best loss: -0.8759423508641557]\n",
      "Best Hyperopt Results:f{best_params}\n",
      "writing to json file....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "train_df = preprocess_data(train)\n",
    "\n",
    "X = train_df.drop('Exited', axis=1)\n",
    "y = train_df['Exited']\n",
    "\n",
    "# Create transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to apply transformers to the appropriate columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "])\n",
    "\n",
    "space = {\n",
    "    #------XGB-------\n",
    "    'xgb_max_depth': scope.int(hp.quniform('xgb_max_depth', 1, 6, 1)),\n",
    "    'xgb_learning_rate': hp.loguniform('xgb_learning_rate', np.log(0.001), np.log(0.2)),\n",
    "    'xgb_min_child_weight': hp.choice('xgb_min_child_weight', np.arange(1, 6,)),\n",
    "    'xgb_subsample': hp.uniform('xgb_subsample', 0.5, 1.0),\n",
    "    'xgb_n_estimators': scope.int(hp.quniform('xgb_n_estimators', 100, 1000, 1)),\n",
    "    'xgb_colsample_bytree':  hp.uniform('xgb_colsample_bytree', 0.5, 1),\n",
    "    'xgb_reg_alpha': scope.int(hp.uniform('xgb_reg_alpha', 0, 10)),\n",
    "    'xgb_gamma': hp.loguniform('xgb_gamma', -10, 10),\n",
    "\n",
    "    #-----RF---------\n",
    "    'rf_n_estimators': scope.int(hp.quniform('rf_n_estimators', 100, 1000, 1)),\n",
    "    'rf_max_depth': scope.int(hp.quniform('rf_max_depth', 1, 10, 1)),\n",
    "    'rf_min_samples_split': hp.choice('rf_min_samples_split', np.arange(2, 11)),\n",
    "    'rf_min_samples_leaf': hp.choice('rf_min_samples_leaf', np.arange(2, 11)),\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    xgb_clf = XGBClassifier(\n",
    "        max_depth=params['xgb_max_depth'],\n",
    "        learning_rate=params['xgb_learning_rate'],\n",
    "        min_child_weight=params['xgb_min_child_weight'],\n",
    "        subsample=params['xgb_subsample'],\n",
    "        n_estimators=params['xgb_n_estimators'],\n",
    "        colsample_bytree=params['xgb_colsample_bytree'],\n",
    "        reg_alpha=params['xgb_reg_alpha'],\n",
    "        gamma=params['xgb_gamma'],\n",
    "        random_state=SEED\n",
    "        )\n",
    "    \n",
    "    \n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=params['rf_n_estimators'],\n",
    "        max_depth=params['rf_max_depth'],\n",
    "        min_samples_split=params['rf_min_samples_split'],\n",
    "        min_samples_leaf=params['rf_min_samples_leaf'],\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "        )\n",
    "    \n",
    "\n",
    "    stacking_clf = StackingClassifier(estimators=[\n",
    "        ('xgb', xgb_clf),\n",
    "        ('rf', rf_clf)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5 \n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', stacking_clf)])\n",
    "\n",
    "    # cross-validation strategy \n",
    "    scores = cross_val_score(pipeline, X, y, cv=SKFold(FOLDS), scoring='roc_auc')\n",
    "    print(\"Cross-validation scores:\", scores)\n",
    "    return {'loss': -np.mean(scores), 'status': STATUS_OK}\n",
    "    \n",
    "# Hyperopt optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=1, trials=trials)\n",
    "\n",
    "\n",
    "print(\"Best Hyperopt Results:f{best_params}\")\n",
    "print(\"writing to json file....\")\n",
    "\n",
    "def write_best_params_to_json(best_params):\n",
    "    # Function to convert NumPy types to Python native types for JSON serialization\n",
    "    def convert_types(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Convert the best parameters using the conversion function\n",
    "    best_params_converted = {k: convert_types(v) for k, v in best_params.items()}\n",
    "\n",
    "    # Write best results to JSON file\n",
    "    with open('hyperopt_results.json', 'w') as fp:\n",
    "        json.dump(best_params_converted, fp, indent=4, sort_keys=True)\n",
    "\n",
    "# Call the function with the best_params dictionary\n",
    "write_best_params_to_json(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.876633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "best_params_formatted = {\n",
    "    \"xgb_max_depth\": int(best_params[\"xgb_max_depth\"]),\n",
    "    \"xgb_learning_rate\": best_params[\"xgb_learning_rate\"],\n",
    "    \"xgb_min_child_weight\": int(best_params[\"xgb_min_child_weight\"]),\n",
    "    \"xgb_subsample\": best_params[\"xgb_subsample\"],\n",
    "    \"xgb_n_estimators\": int(best_params[\"xgb_n_estimators\"]),\n",
    "    \"xgb_colsample_bytree\": best_params[\"xgb_colsample_bytree\"],\n",
    "    \"xgb_reg_alpha\": best_params[\"xgb_reg_alpha\"],\n",
    "    \"xgb_gamma\": best_params[\"xgb_gamma\"],\n",
    "    \n",
    "    \"rf_n_estimators\": int(best_params[\"rf_n_estimators\"]),\n",
    "    \"rf_max_depth\": int(best_params[\"rf_max_depth\"]),\n",
    "    \"rf_min_samples_split\": int(best_params[\"rf_min_samples_split\"]),\n",
    "    \"rf_min_samples_leaf\": int(best_params[\"rf_min_samples_leaf\"]),\n",
    "}\n",
    "\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    max_depth=best_params_formatted[\"xgb_max_depth\"],\n",
    "    learning_rate=best_params_formatted[\"xgb_learning_rate\"],\n",
    "    min_child_weight=best_params_formatted[\"xgb_min_child_weight\"],\n",
    "    subsample=best_params_formatted[\"xgb_subsample\"],\n",
    "    n_estimators=best_params_formatted[\"xgb_n_estimators\"],\n",
    "    colsample_bytree=best_params_formatted[\"xgb_colsample_bytree\"],\n",
    "    reg_alpha=best_params_formatted[\"xgb_reg_alpha\"],\n",
    "    gamma=best_params_formatted[\"xgb_gamma\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=best_params_formatted[\"rf_n_estimators\"],\n",
    "    max_depth=best_params_formatted[\"rf_max_depth\"],\n",
    "    min_samples_split=best_params_formatted[\"rf_min_samples_split\"],\n",
    "    min_samples_leaf=best_params_formatted[\"rf_min_samples_leaf\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_clf),\n",
    "        ('rf', rf_clf)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", stacking_clf)])\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Train the model on the training set\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_pred = final_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate the AUC score on the validation set\n",
    "val_auc_score = roc_auc_score(y_val, y_val_pred)\n",
    "print(f\"Validation AUC: {val_auc_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_prob = final_pipeline.predict_proba(test)[:, 1]\n",
    "\n",
    "submission_df = pd.DataFrame({\"id\": test[\"id\"], \"Exited\": test_pred_prob})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "submission_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggles4e01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
